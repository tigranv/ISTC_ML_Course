{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Language Modeling** is the task of predicting\twhat word comes next.\n",
    "\n",
    "<img src=\"img/predict_next.png\" style=\"height: 150px;\"/>\n",
    "More formally: given a sequence o words $x^{(1)}, x^{(2)}, \\dots,    x^{(t)}$, compute the probability distribution of the next word $x^{(t+1)}$:\n",
    "$$P(x^{(t+1)}=w_j\\;|\\;x^{(t)}, \\dots, x^{(1)})$$\n",
    "where $w_j$ is a word in the vocabulary $V = \\{w_1, \\dots, w_{|V|}\\}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everyday use of Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "<td> <img src=\"img/use1.png\" style=\"height: 250px;\"/> </td>\n",
    "<td> <img src=\"img/use2.png\" style=\"height: 250px;\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram Language Models\n",
    "First we make a simplifying assumption: $\\large x^{(t+1)}$ depends only on the preceding $ (n-1)$ words\n",
    "$$\\large P(x^{(t+1)}\\;|\\;x^{(t)}, \\dots, x^{(1)}) = P(x^{(t+1)}\\;|\\;x^{(t)}, \\dots, x^{(t-n+2)})=$$\n",
    "$$\\large = \\frac{P(x^{(t+1)}, x^{(t)}, \\dots, x^{(t-n+2)})}{P(x^{(t)}, \\dots, x^{(t-n+2)})} \\approx$$\n",
    "$$\\large \\approx \\frac{count(x^{(t+1)}, x^{(t)}, \\dots, x^{(t-n+2)})}{count(x^{(t)}, \\dots, x^{(t-n+2)})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text\n",
    "You can also use a Language Model to **generate text**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "<td> <img src=\"img/generate.png\" style=\"height: 300px;\"/> </td>\n",
    "<td> <img src=\"img/generate2.png\" style=\"height:300px;\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A fixed-window neural Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![neural language model](img/fixed_window_example.png)\n",
    "![neural language model](img/nlm_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Improvements over** n-gram LM:\n",
    "* No sparsity problem\n",
    "* Model size is $O(n)$ not $O(exp(n))$\n",
    "\n",
    "Remaining **problems**:\n",
    "* Fixed window is *too small*\n",
    "* Enlarging window enlarges $W$ \n",
    "* Window can never be large enough!\n",
    "* Each $x^{(i)}$ uses different rows of. We *don’t share weights* across the window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurent Neural Networks  (RNN)\n",
    "![rnn](img/rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Language Model\n",
    "![RNNLM](img/rnn_lm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN **Advantages**:\n",
    "* Can process any length input\n",
    "* Model size doesn’t increase for longer input\n",
    "* Computation for step t can (in theory) use information from many steps back\n",
    "* Weights are shared across timesteps: representations are shared \n",
    "\n",
    "RNN **Disadvantages**:\n",
    "* Recurrent computation is slow\n",
    "* In practice, difficult to access information from many steps back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a RNN Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True next word: $\\large y^{(t)} = x^{(t+1)}$ (one-hot ecoded vector)\n",
    "<br>\n",
    "Predicted Probability distribtion: $\\large \\hat{y}^{(t)}$ (sum to one)\n",
    "<br>\n",
    "Loss function on step t is usual **cross-entropy** between $y^{(t)}$ and $\\hat{y}^{(t)}$:\n",
    "\n",
    "$$\\large J^{(t)}(\\theta) = - \\sum_j y_j^{(t)} ln(\\hat{y}_j^{(t)})$$\n",
    "$$\\large \\mathcal L(\\theta) = \\frac{1}{T}\\sum_{t=1}^T J^{(t)}(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs can be used for tagging\n",
    "e.g. part-of-speech tagging\n",
    "![tagging](img/rnn_tagging.png)\n",
    "\n",
    "# RNNs can be used for sentence classification\n",
    "e.g. sentiment classification\n",
    "![classification](img/rnn_encode.png)\n",
    "\n",
    "# RNNs can be used to generate text\n",
    "e.g. speech recognition, machine translation, summarization, image captioning\n",
    "![speech](img/rnn_speech.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition of CNN and RNN\n",
    "![arcitecture](img/convplusrnn.png)\n",
    "<br>\n",
    "<br>\n",
    "![results](img/captioning.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More reading\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "<br>\n",
    "[Recurrent Neural Networks Tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
