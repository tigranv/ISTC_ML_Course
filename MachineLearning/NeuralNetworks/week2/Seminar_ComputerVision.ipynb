{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Recognition\n",
    "\n",
    "![](./img/classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding Box Regression\n",
    "\n",
    "![](./img/bbreg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "\n",
    "![](./img/seg1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation\n",
    "\n",
    "![](./img/seg2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "<td> <img src=\"img/depthcol.jpeg\" style=\"height: 200px;\"/> </td>\n",
    "<td> <img src=\"img/kernel_convolution.jpg\" style=\"height: 350px;\"/> </td>\n",
    "</tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spatial arrangement**. Three hyperparameters control the size of the output volume: the depth, stride and zero-padding:\n",
    "1. First, the **depth** of the output volume is a hyperparameter: it corresponds to the number of **filters** we would like to use, each learning to look for something different in the input. For example, if the first Convolutional Layer takes as input the raw image, then different neurons along the depth dimension may activate in presence of various oriented edges, or blobs of color. We will refer to a set of neurons that are all looking at the same region of the input as a depth column (some people also prefer the term fibre).\n",
    "2. Second, we must specify the **stride** with which we slide the filter. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2 then the filters jump 2 pixels at a time as we slide them around. This will produce smaller output volumes spatially.\n",
    "3. As we will soon see, sometimes it will be convenient to pad the input volume with zeros around the border. The size of this **zero-padding** is a hyperparameter. The nice feature of zero padding is that it will allow us to control the spatial size of the output volumes (most commonly as weâ€™ll see soon we will use it to exactly preserve the spatial size of the input volume so the input and output width and height are the same).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional layer\n",
    "* Accepts a volume of size $W_1 \\times H_1 \\times D_1$\n",
    "* Requires four hyperparameters:\n",
    "    * Number of filters $K$,\n",
    "    * their spatial extent $F$,\n",
    "    * the stride $S$,\n",
    "    * the amount of zero padding $P$.\n",
    "* Produces a volume of size $W_2 \\times H_2 \\times D_2$ where:\n",
    "    * $W_2=(W_1-F+2P)/S+1$\n",
    "    * $H_2=(H_1-F+2P)/S+1$ (i.e. width and height are computed equally by symmetry)\n",
    "    * $D_2=K$\n",
    "* We have $F \\cdot F \\cdot D_1$ weights per filter, for a total of $(F \\cdot F \\cdot D_1)\\cdot K$ weights and $K$ biases.\n",
    "* In the output volume, the d-th depth slice (of size $W_2 \\cdot H_2$) is the result of performing a valid convolution of the d-th filter over the input volume with a stride of $S$, and then offset by d-th bias.\n",
    "<br>\n",
    "\n",
    "A common setting of the hyperparameters is $F=3,S=1,P=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a running demo of a CONV layer. Since 3D volumes are hard to visualize, all the volumes (the input volume (in blue), the weight volumes (in red), the output volume (in green)) are visualized with each depth slice stacked in rows. The input volume is of size $W_1=5,H_1=5,D_1=3$, and the CONV layer parameters are $K=2,F=3,S=2,P=1$. That is, we have two filters of size $3 \\times 3$, and they are applied with a stride of $2$. Therefore, the output volume size has spatial size $(5 - 3 + 2)/2 + 1 = 3$. Moreover, notice that a padding of $P=1$ is applied to the input volume, making the outer border of the input volume zero. The visualization below iterates over the output activations (green), and shows that each element is computed by elementwise multiplying the highlighted input (blue) with the filter (red), summing it up, and then offsetting the result by the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/convolution.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned filters\n",
    "![](./img/weights.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input volume size: $W$ \n",
    "* Filter/kernel size: $F$ \n",
    "* Stride: $S$\n",
    "* Zero padding on the border: $P$ \n",
    "\n",
    "**spatial size of the output volume:** $(W - F + 2P)/S + 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/stride.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that** hyperparameters have mutual constraints: spatial size of the output volume must be an integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upscale\n",
    "\n",
    "![](./img/upscale.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer\n",
    "It is common to periodically insert a Pooling layer in-between successive Conv layers. Its function is to reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX/AVERAGE operation. The most common form is a pooling layer with filters of size $2 \\times 2$ applied with a stride of $2$ downsamples every depth slice in the input by $2$ along both width and height, discarding $75%$ of the activations. Every MAX operation would in this case be taking a max over 4 numbers (little 2x2 region in some depth slice). The depth dimension remains unchanged. More generally, the pooling layer:\n",
    "\n",
    "* Accepts a volume of size $W_1 \\times H_1 \\times D_1$\n",
    "* Requires two hyperparameters:\n",
    "    * their spatial extent $F$,\n",
    "    * the stride $S$,\n",
    "* Produces a volume of size $W_2 \\times H_2 \\times D_2$ where:\n",
    "    * $W_2=(W_1-F)/S+1$\n",
    "    * $H_2=(H_1-F)/S+1$\n",
    "    * $D_2=D_1$\n",
    "* Introduces zero parameters since it computes a fixed function of the input\n",
    "* Note that it is not common to use zero-padding for Pooling layers\n",
    "<br>\n",
    "There are only two commonly seen variations of the max pooling layer found in practice: A pooling layer with $F=3,S=2$ (also called overlapping pooling), and more commonly $F=2,S=2$. Pooling sizes with larger receptive fields are too destructive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![maxpool](img/maxpool.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deconvolution\n",
    "\n",
    "![](./img/deconv.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Convolutional\n",
    "\n",
    "![](./img/segnet.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
